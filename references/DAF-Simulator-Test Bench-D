
### **Document 2: High-Level Design (HLD)**

**Project:** DAF Simulator Test Bench (DAF-TB)
**Version:** 1.0
**Date:** November 7, 2025
**Related SRR:** DAF-TB-SRR-v1.0

-----

### **1.0 Introduction**

This High-Level Design (HLD) document outlines the software architecture for the DAF Simulator Test Bench (DAF-TB), as defined in the **DAF-TB-SRR-v1.0**. This design provides the *how* to the SRR's *what*, detailing the components, interfaces, and data models required to build a unified, "pluggable" benchmarking framework.

### **2.0 System Architecture: The Pluggable Interface**

The core of the DAF-TB architecture is a **Component-Based Software Engineering (CBSE)** design.[31, 32, 33, 34, 35, 36, 37, 38] This design promotes separation of concerns and fulfills requirement **NFR-1 (Modularity)**.

This architecture is achieved using two primary software design patterns:

1.  **Abstract Base Class (ABC) Pattern** [39, 40, 41, 42, 43]: We will define a formal `IDAFPlant` abstract class. This class acts as the "socket" or "contract" (fulfilling **FR-1.1**). The Test Harness will *only* interact with objects that conform to this interface, inspired by established multiphysics frameworks.[30, 22, 34, 27]
2.  **Facade Pattern** [44]: Each of the five DAF engines will have a "Wrapper" or "Adapter" class that *inherits* from `IDAFPlant`. This Wrapper acts as a Facade, hiding the complex, engine-specific implementation details (e.g., file I/O, subprocess calls, C++/Python API) from the Test Harness.[44] This fulfills **FR-1.2**.

#### **2.1 Architectural Diagram**

```
+---------------------------+

| Test Harness (Driver) |
| (daf_benchmark.py) |
+---------------------------+

| - Loads TestConfiguration |
| - Calls plant.run() |
| - Calls plant.get_metrics()|
+-------------^-------------+
|
     (Interacts ONLY with)
|
+-------------v-------------+

| <<Interface>> (ABC) |
| IDAFPlant |
+---------------------------+

| + setup(config) |
| + initialize() |
| + run() |
| + get_metrics() |
| + get_field_data() |
| + finalize() |
+-------------^-------------+
|
     (Is Implemented By)
|
+---------------------------+---------------------------+---------------------------+

| Engine1_Wrapper(..) | Engine2_Wrapper(..) | EngineN_Wrapper(..) |
| (Python/Facade) [44] | (C++/Facade) [44] | (OpenFOAM/Facade) [45] |
+---------------------------+---------------------------+---------------------------+

| - _call_python_api() | - _call_binary() | - _run_openfoam_solver() |
| - _parse_hdf5_results() | - _parse_log_file() | - _parse_vtk_results() |
+---------------------------+---------------------------+---------------------------+
```

### **3.0 Component Breakdown**

#### **3.1 Test Harness (The "Driver")**

  * **Module:** `daf_benchmark.py`
  * **Description:** The main executable script that orchestrates the entire benchmark (fulfills **FR-2.1**).
  * **Responsibilities:**
      * Parses command-line arguments (e.g., path to test config, list of engines to run).
      * Loads the `TestConfiguration` data model (**FR-4.1**).
      * Dynamically instantiates the requested `EngineX_Wrapper` objects as `IDAFPlant`.
      * Iterates through each test case (T1-T5) and each engine, calling the `IDAFPlant` methods in the correct order (setup, initialize, run, get\_metrics, finalize).
      * Aggregates results from `get_metrics()` into a `results_database` (e.g., a list of dictionaries).
      * Calls the `MetricsModule` to export the final database to disk (fulfills **FR-5.3**).

#### **3.2 `IDAFPlant` (The Interface)**

  * **Module:** `interfaces.py`
  * **Description:** A Python **Abstract Base Class (ABC)** [39, 43] defining the "contract" for all DAF engines (fulfills **FR-1.1**).
  * **API Definition (Python):**
    ```python
    from abc import ABC, abstractmethod

    class IDAFPlant(ABC):
        """Abstract Base Class defining the standard interface for all DAF simulation engines."""
        
        @abstractmethod
        def setup(self, configuration: dict) -> bool:
            """Configures the simulation with standardized stimuli."""
            pass

        @abstractmethod
        def initialize(self) -> bool:
            """Initializes the simulation state, loads mesh, etc."""
            pass

        @abstractmethod
        def run(self) -> bool:
            """Runs the entire simulation to completion."""
            pass

        @abstractmethod
        def get_metrics(self) -> dict:
            """Returns all performance metrics (FR-5.1, FR-5.2) in a standardized dictionary."""
            pass
            
        @abstractmethod
        def get_field_data(self, variable_name: str) -> object: # e.g., np.ndarray
            """Returns a full data field (e.g., velocity) for validation."""
            pass

        @abstractmethod
        def finalize(self) -> None:
            """Cleans up all resources."""
            pass
    ```

#### **3.3 Engine Wrappers / Adapters (The "Plugs")**

  * **Module:** `engines/engine_1.py`, `engines/engine_2.py`,...
  * **Description:** A set of concrete classes, one for each of the five DAF engines, implementing the `IDAFPlant` interface (**FR-1.2**).
  * **Example (`Engine1_Wrapper(IDAFPlant)`):**
      * `setup()`: Reads the standard `configuration` dict and writes a proprietary `engine1_input.dat` file.
      * `run()`: Executes `subprocess.run(["./engine1.exe", "engine1_input.dat"])`.
      * `get_metrics()`: Parses `engine1_output.log` and `results.csv` to build and return the standardized metrics dictionary.

#### **3.4 V\&V Module (The "Conformance Tester")**

  * **Module:** `verification.py`, `validation.py`
  * **Description:** A collection of functions and classes to perform the conformance tests from **FR-3**.
  * **Components:**
      * `MMS_Verifier`: A class that implements the **Method of Manufactured Solutions**.[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] It will use a symbolic math library (e.g., SymPy) to generate source terms [2], modify the `TestConfiguration` to include them, and run the `TestHarness` on a series of refined meshes to compute the convergence order.
      * `PBM_Verifier`: A function that runs the `TestHarness` on the 0D PBM test cases (T3) and compares the output `get_field_data()` against known analytical solutions for constant, sum, and product kernels.
      * `Validator`: A class that loads external experimental "ground truth" data (e.g., PIV/LDA data [20, 21, 22, 23, 24, 25, 27, 28], FBRM data) and compares it against the simulation output from `get_field_data()`, reporting error norms.

#### **3.5 Test Bench & Stimuli**

  * **Location:** `/test_cases/`
  * **Description:** A directory containing all standardized inputs for the benchmark suite (**FR-4.1**).
  * **Structure:**
      * `/test_cases/T1_Hydro/config.json` (Stimuli file)
      * `/test_cases/T1_Hydro/mesh/` (Standardized mesh files)
      * `/test_cases/T2_Multiphase/config.json`
      * `/test_cases/T2_Multiphase/ground_truth/piv_data.csv` [20]
      * `/test_cases/T3_PBM/config.json`
      * `/test_cases/T3_PBM/ground_truth/analytical_solution.csv` [46]
      * ...etc. for T4 (RTD [47]) and T5 (Full-System).

#### **3.6 Metrics & Reporting**

  * **Module:** `metrics.py`
  * **Description:** Handles the collection and storage of all benchmark data (fulfills **FR-5**).
  * **Responsibilities:**
      * Defines the standardized metric keys (e.g., `sci_particle_removal_eff`, `comp_wall_clock_sec`).
      * Provides utilities to save the `results_database` (from the Harness) to a persistent, queryable format (e.g., a single HDF5 file, or a collection of JSON files).
      * Provides export functions (e.g., `export_to_csv()`) for use in external plotting and analysis tools.

### **4.0 Data Models (High-Level Schema)**

1.  **`TestConfiguration` (JSON Schema):**

      * `test_name`: (string) e.g., "T2\_Multiphase\_Standard\_k-Epsilon"
      * `description`: (string)
      * `mesh_file`: (string) "path/to/mesh.msh"
      * `stimuli`: (object)
          * `hydrodynamics`: { `inlet_flow_rate`: 1.5, `recycle_ratio`: 0.1,... }
          * `pbm`: { `influent_psd`: [...], `kernel_coeffs`: {...},... }
          * `solver`: { `timestep`: 0.01, `convergence_limit`: 1e-6 }
      * `ground_truth_files`: (object) { `piv`: "path/to/piv\_data.csv", `fbrm`: "path/to/fbrm.csv" }

2.  **`BenchmarkResult` (JSON Schema, one per run):**

      * `engine_name`: (string) "Engine1\_OpenFOAM"
      * `test_name`: (string) "T2\_Multiphase\_Standard\_k-Epsilon"
      * `timestamp`: (string) ISO 8601
      * `scientific_metrics`: {
          * `particle_removal_eff`: 0.95,
          * `velocity_error_l2`: 0.045,
          * `mass_conservation_error_pct`: 0.001,
          * ... (all metrics from **FR-5.1**)
      * `computational_metrics`: {
          * `wall_clock_sec`: 3600.5,
          * `cpu_hours`: 28804.0,
          * `peak_ram_gb`: 120.5,
          * `converged`: true,
          * ... (all metrics from **FR-5.2**)
      * `run_log`: (string) "Full stdout/stderr from the run."

### **5.0 Integration & Workflow (UC-2 Example)**

1.  A **Researcher** executes: `python daf_benchmark.py --config /test_cases/T2_Multiphase/config.json --engines engine1,engine3`
2.  The **Test Harness** loads `config.json`.
3.  The **Test Harness** instantiates `Engine1_Wrapper()` as `plant`.
4.  The **Test Harness** calls `plant.setup(config)`. The `Engine1_Wrapper` creates its specific input files.
5.  The **Test Harness** calls `plant.run()`. The `Engine1_Wrapper` executes the OpenFOAM binary.
6.  The **Test Harness** calls `plant.get_metrics()`. The `Engine1_Wrapper` parses the simulation output and returns a `BenchmarkResult` dictionary.
7.  The **Test Harness** calls `plant.finalize()`.
8.  The **Test Harness** saves the result and repeats steps 3-7 for `Engine3_Wrapper()`.
9.  The **Test Harness** exports the final `results_database` to `benchmark_run_T2_.....csv`.
